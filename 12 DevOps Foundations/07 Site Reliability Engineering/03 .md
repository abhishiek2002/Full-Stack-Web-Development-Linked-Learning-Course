Building for reliability: Practice
- As I like to say, "Dev comes from school, but Ops comes from the street." Developers tend to come from computer science backgrounds, but a lot of system administrators are self-taught. They learn from the real world of running large scale systems. SRE is in large part bringing that hard one ops experience to the table and turning it from manual fiddling to disciplined engineering. You are now about to witness the strength of street knowledge. The hard truth is that all systems fail. Codes often written with the assumption that failure of the underlying systems is if not impossible, at least very unusual and probably should result in some manual intervention. But failure and slowdowns, which are as much of a threat as outages, are common. In a modestly complex system individual components are failing all the time, as every operations engineer who's spent time looking through log files knows, and this doesn't always cause an outage. System components are like slices of Swiss cheese stacked on top of each other. If the holes in the slices don't line up, then things keep working. But when they do, then users experience problems. Medical Doctor Richard Cook wrote a key paper entitled, "How Complex Systems Fail", used first for resilience engineering and medicine, but it's been widely applied to tech as well. Its key findings are echoed throughout our discussion of SRE. Like, changes introduce new forms of failure. Complex systems contain changing mixes of failures latent within them. All complex systems are always running in degraded mode. In tech, we often refer to the number of nines of availability A system has, for example, a system with 99.9% uptime is said to have three nines of uptime, which is the equivalent of 8.77 hours of downtime a year. A system with five nines is 99.999% available only 5.26 minutes of downtime per year. For too long, every bit of it effort was put into preventing problems from happening in the first place. The problem is this is incredibly difficult to do. Pushing more and more money at highly available systems is a losing game. I spent a lot of time in my career listening to excuses about why the system that, "doesn't have any single point of failure", still went down. Instead, we approach our systems with the idea of resilience. Resilience is the intrinsic ability of a system to maintain or regain a dynamically stable state, which allows it to continue operations after a major mishap and or in the presence of a continuous stress. Resilience Engineering focuses on understanding how complex adaptive systems respond to a surprise or disruption. So instead of trying to engineer a single perfect component, we use tools like; redundancy, running multiple identical copies of a server application container or the like, so that we can maintain service if we lose one. We use load balancers or other traffic shaping devices to send user traffic to the healthy parts of the system. Then from a performance perspective, we create automatic scaling, so that when there's too much load, we don't need to get a bigger server. We can just add more servers or other components to grow to accommodate load. And then from there you can automate further failover and recovery activity. A Kubernetes installation, for example, runs redundant copies of both its core services and each of its applications across multiple servers, has health checking built in, performs automatic failover when one goes down, replicates the cluster state in multiple places, all kinds of coping mechanisms to keep its system up and responding in the face of single or even multiple component failures. But there's no perfect mouse trap and that's not even your complete system. People are part of your system. The actions of people can break parts of your system and the actions of people can help maintain your system's health. An important part of resilience engineering is realizing that all systems are sociotechnical. In other words, they consist of the computers, but also the people who create, maintain and use them. Don't spend so much time planning for the perfect system uptime that you fail to acknowledge, it's really going to be at least partially broken all the time. And expert practitioners are needed to intervene. SREs build applications to maintain and fix the systems they work on. In fact, a common rubric is that SREs should be spending at least half of their time developing tools and not just fixing things manually. Writing runbooks ways to safely intervene in system operations, better ways to feedback monitoring into controlling the system. And remember ensuring your services running isn't just for operations engineers, developers are experts on their code and should be involved in resolving problems. This means developers are on call to help when their code goes awry. "You write it, you run it." Is a common saying. At Google, the dev team support their own services until they convince the SRE team that their service is stable and has enough monitoring and tooling to be supported by another team in production. Developers should learn to use their debugger, profiler and application performance management tools in a runtime environment. You're a critical part of the resilience picture as well.