Your DevOps SRE toolchain
- There are two parts to a site reliability engineering tool chain. - The first part, building for reliability, is hard to give general guidance on. It's going to be very different based on programming language and tech stack, and it's less about tools and more about libraries and development techniques - Right. The important thing to know is that there are books that describe these techniques, tools and libraries like Java's Resilience4j, and dev and ops engineers need to collaborate at design time to determine what kind of built-in resilience approach to use. - The second part to your tool chain, operational feedback, uses a fairly common set of observability and incident response tooling. - Yeah, this is a very rich space with offerings for every kind of monitoring and suites to combine them. - [Presenter in Red] You have SaaS offerings like Datadog, Honeycomb, and SumoLogic. - You have open source tools like Nagios, Grafana, and Prometheus. - And commercial software like Solarwinds and Splunk. - In fact, of all the tooling areas in DevOps, this is the one where there really might be too many options. - Well, might be second after Kubernetes. - Okay, that's accurate. Anyway, not only are there a dizzying amount of tool options, but also, this is a part of your system that it's really the worst to design completely upfront. - Why? While there are some commonalities, the specific service you build is going to have its own observability needs that you won't fully understand until you've seen it work in production. - [James] Observability is a key place to use your lean techniques. Our five areas of tooling are synthetic checks, system and application metrics, end-user performance, system and application logs, and security monitoring. - Instead of guessing which ones we need to invest in, let's use our lean build measure learn approach when creating observability. First, build. Create a minimum viable monitoring stack, synthetic monitors on a couple endpoints, system monitoring to get basic stats, performance latency from logs, and so on. Implement the quickest, cheapest option for all five areas. - Okay, second. Measure. You know, just start getting metrics from each of those areas of monitoring. - And third, learn. Analyze the application stack with the monitoring in place and then see where you need more to solve your problems. Where are you needing more detailed system metrics? Can the app developers tell what's going on in their app from their own logs, or do they need to iterate and improve them? And then repeat this cycle and go deeper as needed. - This lets you hone in on what kind of information really helps you support your service, because monitoring you don't use, that's waste. I worked on one team where we were so vigorous about our monitoring that we realized a full 30% of our system load was coming from our own monitoring. That was not so great. - James and I were working together in one place turning a piece of packaged software into an online service. The developer was new to the web world and his application logs were initially terrible, but that was okay, because it allowed us to learn. The third way of DevOps at work. - Yeah. As we worked towards making the service production ready and we'd have an issue, we'd pull him in and show him his own logs and say, hey, what's going on here? And he couldn't tell, but then he'd improve his logs. Next we said, now, if you improve them so much that one of us knows what's going on, then we'd have to call you in here less. And the logging grew strong with that one. - Now we're measuring. But are we sharing? As you build out your observability stack, remember that an SRE or ops engineer is not the only person who can benefit from feedback from your production applications. - Yeah. Developers can use error and performance information to further improve their app. You know, product managers and business decision makers, they need to know how things are going, what to budget for, and so on. - And go beyond their built-in reports and dashboards, and perform custom development where needed. I was working at a SaaS provider that deeply integrated into many major retail websites. We built a custom visualization that looked like the whiteboard picture that people across the business generally understood our service to look like and marked it up with error rates and performance that would turn, you know, green, yellow, or red based on status. Every sales rep knew where their big customer was, for example, on cluster five, so it made the monitoring meaningful to them. At a glance, people across the company could reassure themselves that things were fine. And when clusters did begin to run hot, there was an immediate gathering of the right people to be involved in the incident. - Okay. Now, speaking of incident response, there are also good tools for incident response and management, and runbook automation to use for, you know, like, routine activities. Pagerduty, for example, is a SaaS solution that takes basic alerts from observability tools and allows for rich on-call scheduling and escalation workflows to allow for clean incident response. There's others, like VictorOps and OpsGenie. - Rundeck is a runbook automation tool that I've used to set up both emergency and routine maintenance activities. There are others, like Ansible Tower and StackStorm. I usually use Rundeck myself, because it has open source, commercial, and SaaS options. - And finally, there are status page tools you can use to transparently communicate outages to users, like Atlassian Statuspage, Status.io, and some others. - [Presenter in Red] There's a lot of SRE tooling, and while those are the primary areas, there are many more, and this is a place where organizations end up writing a lot of custom tooling to fit their specific use case. - Remember, the key is to keep it simple and keep your people in mind, and then select and iterate a tool chain that best lets your entire team collaborate around supporting your environment.