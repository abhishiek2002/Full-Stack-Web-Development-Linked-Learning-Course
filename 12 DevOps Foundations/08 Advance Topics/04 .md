DevOps and chaos engineering
- Chaos. That doesn't sound great when we're talking about our IT systems. might be fun as an alignment when we're playing Dungeons and Dragons and want our characters to act up, but we want to eliminate chaos from our workplace, right? Well, there's an advanced topic in resilience called chaos engineering. Chaos engineering is the discipline of experimenting on a system in order to build confidence in the system's capability to withstand real world conditions in production. In other words, creating deliberate adversity for your system. What does that even mean? Well, Netflix introduced the ideas we know it today with a tool that they called the Chaos Monkey. Netflix was running huge clusters of cloud systems for millions of users. They told their technical staff to build a system that was resilient in the face of problems. It should keep on working if any particular server or network link or other component failed. So, they'd write and test their code, including the operational code designed to make the whole system stay up and working with that in mind. But they realized pretty quickly that the fault testing they were doing as part of their usual CICD process wasn't enough and that it couldn't be, they couldn't put together a test system consisting of all of the thousands of other components in the Netflix ecosystem and couldn't emulate the craziness that the real production system faced in contact with millions of internet users every day. They were trying to make a resilience system but didn't have a way to validate that except for waiting for real production problems to happen, which is always a very upsetting time for everyone. So they made the Chaos Monkey. The Chaos Monkey is an angry monkey. It goes into their production system and breaks something itself. That's right, what better way to force every single tech team to make sure that the Netflix system won't be degraded when a server goes down than to have the Chaos Monkey go in and smash a server every once in a while? And it worked great. It helped them continually improve the resilience of their system by creating failures and learning from them before failures inevitably showed up on their own. And so, we have chaos engineering. Now, it's controlled chaos. It's not really as simple as shock the monkey. In Chaos engineering, you construct experiments that you want to validate via some kind of fault injection. You do fault testing in your development process, too, of course, this isn't an excuse to be sloppy. Testing and production is cool, but not testing and development is just foolish. If you believe a given fault should be automatically remediated in your environment, then chaos experiments can help validate that. But it's also used for faults that require human intervention. Activities called Game Days can be held where a significant fault is either emulated or even really created where the human part of incident response is tested as well. Remember, all our systems are sociotechnical systems. If we don't test our incident response, then we shouldn't be surprised when it's poor. James and I worked at a startup building chaos experiments for Kubernetes. Since it's a complex system, there are many different things that can go wrong with it. And unless you try breaking those things, you don't know how your system will respond, and more importantly, what that looks like to a human operator. IT systems aren't binary, at least not above the chip level. What happens when you lose a server or just one application container on a server? Does it recover? How fast does it recover? What happens if that server actually reconnects later? We found that people's assumptions about how their systems would behave were not strongly correlated with how they actually did behave when something broke. Break the network, break the container repository, break DNS. It lets you get past guessing how that will look to experimenting so you really know. It's like automobile crash tests. They don't just build a car they think is going to be sturdy. They break it a hundred times under different scenarios to figure out what happens when it does crash. I like chaos engineering because it's an example of innovative thinking, breaking one of the walls of constraint that we labor under without even thinking about it and saying, "But what could we learn and improve if we did this in production instead?" Chaos Engineering is all about becoming a learning organization, part of our target DevOps culture, and it's a direct application of the three ways of DevOps. Use feedback loops to create learning.