MLOps: Leveraging DevOps to run ML systems
- AI is huge. Huge I tell you. Machine learning is taking everything over. - Okay, well that may be a bit of an exaggeration. - Chat GPT told me you were going to say that. - Okay well, while machine learning's been around for a long time, it's mostly been used by huge social media companies, and hardcore scientists, and engineers up until recently. Now, the boom in generative AI has every single company scrambling to come up with an AI ML strategy. - And of course, while you can play around with web-based AI tools, most of the ways to create business value with AI involve lots of private data, and lots of training of private machine learning models. - Okay, well that sounds like it needs infrastructure and it's a DevOps problem. - It is, but with a little bit of a unique spin on it. And thus the term MLOps was born, combining machine learning and DevOps. - Okay well, what makes MLOps different from just normal DevOps? I mean, you spin up servers, you send out jobs, you monitor them. Sounds like your usual combo of infrastructure as code, continuous deployment and SRE work, right? - There's a lot of truth to that. But there are some areas where MLOps pushes the boundaries of typical DevOps techniques. The biggest thing is that often it's data scientists and not developers who are the primary workload generators. Data scientists often don't understand computer systems as much as ops staff or even app developers, but their work is tightly coupled with the hardware they use because the jobs are so demanding. This means a need for close collaboration and training and empathetic support. - Okay, I see. So, and instead of just moving apps to production, you have data and models to manage too. Okay. - Exactly. In addition to the usual work managing and deploying code and infrastructure, MLOps automates the versioning and management of massive data sets and ML models, which are basically algorithms used to find patterns in that data. The actual machine learning model training workloads are intensive batch jobs, best run on high performance computing, or HPC clusters. HPC clusters are highly optimized systems for closely integrated compute storage and network, often using GPUs for extra power. And powerful means expensive. And then the inference apps you expose for people to use the model need their own build and management, and models and data sets need to go to end user facing production environments with them. - Ah, I see. While the core DevOps concepts apply perfectly to that kind of environment, a lot of the tools and such are oriented around a more traditional kind of database workload. So okay, so that's how MLOps is different. - There's also a matter of results tracking and governance. With most applications, you're looking for them to give you a single correct answer, and you test that it does. AI systems don't work like that. You get different outputs of varying quality, and you're always modifying the data in the model to get better results. So there's a feedback loop there that's richer than did the test case pass. - Ah, okay, and AI apps keep learning from user input once they're trained and put into production, right? - That's right. While usually a lot of the model training happens up front and then end users perform inference, which is a fancy word for, "Hey, AI, write my book report for me," many inference workloads feed back into training as well. Detecting drift in AI predictions and production is a more complicated problem than, "is the right data showing up in my API." Also, the vector databases used to store the model data are large, and they can become even more large and expensive as they grow with long-term inference memory. - Okay. The process sounds similar to a normal development value stream, except you start with a model training, continuous integration loop, then a model deployment process, and then continuous monitoring in production. And alongside that, there are three parallel and interrelated CICD processes for the software, the infrastructure, and the data and models. - That's right. With three kinds of specialists participating, devs, ops and data scientists. I've spent a number of years working at a consultancy where we do MLOps work for some very large customers. While there's all kinds of in-depth technical details for AI ML, the same DevOps concepts make it successful. Automation, measurement and so on. - Okay, that's really great. - Chat GPT told me you would say that, too. AI is here to stay as a fundamental computing pattern and workload, so more and more DevOps professionals will need to understand it in the future. For more on MLOps, look up the MLOps essentials courses here in the library.